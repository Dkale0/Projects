{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc422fd2",
   "metadata": {},
   "source": [
    "## Handwritten letters image recognition using CNN (notMINST dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1cc611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "import splitfolders\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eae92b",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n",
    "Before we can use the data, we need to transform it into a usable state. The steps to do this are:\n",
    "* Split the Dataset\n",
    "* Remove Broken Files\n",
    "* Size normalizing/Centering and transforming data to tensors\n",
    "* Data Loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ea785",
   "metadata": {},
   "source": [
    "### Splitting Dataset\n",
    "Since our dataset has not been split into testing and training sets, we can do so easily using the splitfolder library. The splitfolders.ratio function splits our data with ratios=(.9, .0, .1), each ratio for the proportion of data for the training, validation, and testing set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fea27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitfolders.ratio(\"dataset\", output=\"train_test_split\", seed=1337, ratio=(.9, .0, .1), group_prefix=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4672c1",
   "metadata": {},
   "source": [
    "### Removing Broken Files\n",
    "Some of the files may not be valid png images. We can check the file type by checking if imghdr.what(filepath) returns \"png\". If it does not we remove the file from the testing/training set. We also travese each file in the training/testing sets using nested for loops and function from the os library allowing us to check/change/remove files from the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b61c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "dataset_path = cwd + \"\\\\train_test_split\"\n",
    "train_path = dataset_path + \"\\\\train\"\n",
    "test_path = dataset_path + \"\\\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e377c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imghdr\n",
    "import pathlib\n",
    "\n",
    "for ds in [train_path,test_path]:\n",
    "  for path in os.listdir(ds):\n",
    "      filepath = pathlib.Path(str(ds)+'\\\\'+path)\n",
    "      for file in os.listdir(filepath):\n",
    "          currfilepath = str(filepath)+'\\\\'+file\n",
    "          if imghdr.what(currfilepath) != 'png':\n",
    "              print(f'Found broken file! : {file}')\n",
    "              os.remove(os.path.join(path, currfilepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258a7d8",
   "metadata": {},
   "source": [
    "###  Transforming data\n",
    "We set the size, where height and width is equal to 28 (pixels). We resize each image using the transforms.Resize() function. Next, we transform each image into a tensor using the transforms.ToTensor() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a95a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darsh\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: [WinError 126] The specified module could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "h = w = 28\n",
    "channels = 1\n",
    "batch_size = 64\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),    \n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c72291",
   "metadata": {},
   "source": [
    "### Create Dataloaders\n",
    "DataLoaders simplify preprocessing our data, before we use it for training our model. They can split our dataset into batches and shuffle the data (increases generalization). It is a Map-style dataset, which means it provides random-access capabilities, which allow for easier parallel loading (useful for large datasets like ours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "961ce2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33960400",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = torchvision.datasets.ImageFolder(root=train_path, transform=train_transform)\n",
    "tst_data = torchvision.datasets.ImageFolder(root=test_path, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "204a33aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(tr_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(tst_data, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f24eb6",
   "metadata": {},
   "source": [
    "### Check if data correctly loaded\n",
    "We print out 4 labels and their respective images to check if our preprocessing was successful. We can also see that the letters may be of different shape/size/fonts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7da22636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    C     B     I     F\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "classes = os.listdir(train_path)\n",
    "print(classes) # only 10 alphabets\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images[:4]))\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bf168aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d0d400",
   "metadata": {},
   "source": [
    "### Building our Model\n",
    "The first layer of a model has the parameter in_channels=3 since the png image is a RGB image, out_channel is the number of nodes for the current layer, which we chose to be 32. After this we apply the ReLU activation function on the output of the previous layer. Layers 3&4, 5&6 repeat the steps above, where the in_channel is the out_channel of the last convulutional layer and the out_channels is half of the current in_channels. Next, layer 7 applies the Max pooling function on the output, reducing the size of the data. This reduces model fitting time and helps to avoid overfitting. The next layer flattens the matrix into a vector. Layers 9,10,11 apply a linear transformation reducing the output into a vector of size 10. If we apply the softmax function on these we will get the probabilities for the image being classified to each class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80c57e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense cnn\n",
    "model = torch.nn.Sequential(\n",
    "    \n",
    "    # layers 1-6\n",
    "    torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=0),\n",
    "    torch.nn.ReLU(),  \n",
    "    torch.nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=0),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=0),\n",
    "    torch.nn.ReLU(),\n",
    "    \n",
    "    # layers 7,8\n",
    "    torch.nn.MaxPool2d(kernel_size=2),\n",
    "    torch.nn.Flatten(),\n",
    "    \n",
    "    # layers 9,10,11\n",
    "    torch.nn.Linear(968, 512),\n",
    "    torch.nn.Linear(512, 128),\n",
    "    torch.nn.Linear(128, 10), \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb03f3",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Our loss function will be the multiclass class entropy function. the optimizer uses the Adam model to minimize this loss, we chose a learning rate similar to the one we used for the MINST dataset since they are both similar in style. We determine that 10 epochs provides a good balance between over/under fitting the model, given our learning rate. Next for each batch of data from the train_dataloader we first set our optimizers weights to zero with zero_grad(). Next we get the predicted probabilites (transformed by torch.nn.linear) for each class for that batch of data. We then calculate loss based on this output and the true label for the training data set. Next, loss.backward() is the back propgation step that finds the updates for our parameters and optimizer.step() actually updates the parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d063441",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7d239aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0189, grad_fn=<NllLossBackward0>)\n",
      "1 tensor(0.0371, grad_fn=<NllLossBackward0>)\n",
      "2 tensor(0.0187, grad_fn=<NllLossBackward0>)\n",
      "3 tensor(0.0019, grad_fn=<NllLossBackward0>)\n",
      "4 tensor(0.2212, grad_fn=<NllLossBackward0>)\n",
      "5 tensor(0.0017, grad_fn=<NllLossBackward0>)\n",
      "6 tensor(0.0911, grad_fn=<NllLossBackward0>)\n",
      "7 tensor(2.7942e-05, grad_fn=<NllLossBackward0>)\n",
      "8 tensor(0.6959, grad_fn=<NllLossBackward0>)\n",
      "9 tensor(0.0003, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# model.to(device)\n",
    "for epoch in range(10):\n",
    "    for data in train_loader:\n",
    "        image, label = data\n",
    "#         image = image.to(device)\n",
    "#         label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e995cef",
   "metadata": {},
   "source": [
    "### Testing the Model\n",
    "The torch.no_grad() function disables gradient calculations over the weights (note: this will NOT affect the accuracy of the model) reducing memory consumption. Next we get all the batches of data from the test_loader, and get the raw output from the model (after the CrossEntropyFunction applies the softmax on the raw output of the CNN we get the probabilites) for each class. Then using the torch.max() function we get the maximum value from the tensors (returns both value and index). We increment total number of imagess (this will be 10,000 at the end) and also increment the number of images correctly classified for that particular batch of images. Finally we calculate the test accuracy by dividing the total correctly classified images by total images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2195dcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9292553191489362\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data[0], data[1]\n",
    "        outputs = model(images)\n",
    "        predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted[1] == labels).sum().item()\n",
    "print(\"Test accuracy: \", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc8c45a",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "While our crossentropy was very low when training 0.003, our final accuracy on the test set is lower than expected (0.93). This can be attributed to a multitude of factors, such as possible overfitting of the model, small training data set size, optimization/loss function, or just randomness. One way to improve this accuracy would be to simply try a larger variety of (suitable) combinations parameter/model types. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
