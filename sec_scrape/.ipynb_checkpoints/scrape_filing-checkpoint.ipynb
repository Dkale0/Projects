{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC Filings Web Scraping and Parsing\n",
    "\n",
    "In this project we will make extracting information from the SEC Filings (10-K and 10-q) easily accessible and automate the process of retrieving only necessery data since these filings can be filled with information that may not be relavant. There are two classes we create to encapsulate the fuctionality for web scraping SEC filing from the EGAR Database and Parsing these filings that are in xml form. Note: there is a known [403 Forbidden error](https://github.com/jadchaar/sec-edgar-downloader/issues/77) in 2021 (unfixed, solution explained later) that prevents us from getting the filings in the xml form, which would have been ideal since the format is consistent across different filings for each company and make the extraction of tables very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys ## all keyboard keys imported\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os, time\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import copy\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Filings\n",
    "\n",
    "The Scrape_filing class has the follwing methods:\n",
    "##### __init__(self, driver_path=None, teardown=False)\n",
    "* Sets driver_path to working directory. Note the user must first download a chrome driver from [this](https://chromedriver.chromium.org/downloads) site inorder to use selenium. This will also set download path to the same directory the chrome driver is in + \"\\\\path_files\", where files needed to access these filings will be stored. \n",
    "  \n",
    "##### __exit____exit__(self, exc_type, exc_val, exc_tb)\n",
    "* This method will quit the chrome tab used by selenium once the class is no longer in use. \n",
    "    \n",
    "##### find_file(self, name, path)\n",
    "* This method returns the file path after iterating through the giver directory. Will be used to find files in the path_files directory.\n",
    "  \n",
    "##### find_cik(self, company=\"BlackRock Inc.\")\n",
    "* This method opens the link \"https://www.sec.gov/edgar/searchedgar/cik.htm\" and inputs the arguments given for company and extracts the CIK (Central Index Key or CIK number is a number given to an individual, company, or foreign government by the United States SEC).\n",
    "  \n",
    "##### find_filing_address(self, cik, year=2020, quarter=1, only_10k=False)\n",
    "* The parameters for this method are the CIK number (which we found with the method above), year, quarter, and boolean for if we want the 10k. The EDGAR databse stores the urls for all the companies that filed for a particular quarter in a particular year, on this page from the edgar search api: \"https://www.sec.gov/Archives/edgar/full-index/{year})}/QTR{quarter)}\". We download this file for future use into the folder we created for path files when we instantiate the class. Note that we do not use Selenium in headless mode here, which means this process of extracting the CIK will be slow and inefficient. The reason for using Selenium is for me to get familiarized with this library. From this we must extract the particular filing (10-k/q) that we are looking for given our CIK. There are many intricacies to getting to this step due to the way the EDGAR database is structured, and the comments in our code provide further explanation to this.\n",
    "\n",
    "##### find_10k_address(self, cik, year):\n",
    "* This method simply relies on the method above, however it calls it on the path_files for each quarter of a year since companies may not necessarily release their 10k's (annual filings) in the same quarter.\n",
    "\n",
    "##### def filing_xml_form(self, file_add):\n",
    "* Unfortunately when trying to access the filings in xml form, we get a [403 Forbidden error](https://github.com/jadchaar/sec-edgar-downloader/issues/77). The response from the SEC webmaster email for this issue was to use the form: response = requests.get(url, headers={'User-Agent': 'Company Name:idk@uu.uu) when requesting the json from their api. I did not implement this fix since I am unsure what the case is for somone not affiliated with a company. Instead we will try to parse the html text file for the tables, which will be much harder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrape_Filing(webdriver.Chrome):\n",
    "    def __init__(self, driver_path=None, teardown=False):\n",
    "        if driver_path == None:\n",
    "            driver_path = os.path.abspath(os.getcwd()) ## Gets current working directory\n",
    "        self.driver_path = driver_path\n",
    "        self.teardown = teardown\n",
    "        os.environ['PATH'] += self.driver_path\n",
    "        options = webdriver.ChromeOptions()\n",
    "        #options.add_experimental_option('excludeSwitches', ['enable-logging']) ## stops the warnings related to reading file descriptors logs\n",
    "        \n",
    "        download_path = driver_path + '\\\\path_files'   # set path for when we download the filing to minimize requests sent to server when we parse it \n",
    "        self.download_path = download_path\n",
    "        preferences = {\n",
    "            \"download.default_directory\":download_path\n",
    "            \n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", preferences)\n",
    "       \n",
    "        super(Scrape_Filing, self).__init__(options=options) ## use super to instantiate the webdriver.chrome class\n",
    "        self.implicitly_wait(15)\n",
    "        # self.maximize_window()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb): ## method to close the chrome window\n",
    "        if self.teardown:\n",
    "            self.quit()\n",
    "            \n",
    "    def find_file(self, name, path):\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            if name in files:\n",
    "                return os.path.join(root, name)\n",
    " \n",
    "        \n",
    "    def find_cik(self, company=\"BlackRock Inc.\"):\n",
    "        self.get(\"https://www.sec.gov/edgar/searchedgar/cik.htm\")\n",
    "        company_element = self.find_element_by_id(\"company\")\n",
    "        company_element.send_keys(company)\n",
    "        submit_element = self.find_element_by_class_name(\"search-button\")\n",
    "        submit_element.click() \n",
    "       \n",
    "        try:\n",
    "\n",
    "            table_element = self.find_element_by_css_selector(\"table[summary='Results of CIK Lookup']\")\n",
    "            rows = table_element.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "            td_row = rows[0].find_elements(By.TAG_NAME, \"td\") ## d_row has 2 rows\n",
    "            \n",
    "            pre_list = td_row[1].find_elements(By.TAG_NAME, \"pre\") ## there are 2 <pre> tags in 2nd rows\n",
    "            cik = pre_list[1].find_elements(By.TAG_NAME, \"a\")[0].text ## we access the very first <pre> tag and the first a tags text is our cik \n",
    "            return cik\n",
    "        except Exception as e:\n",
    "            print(\"try a different name\")\n",
    "            \n",
    "    def find_filing_address(self, cik, year=2020, quarter=1, only_10k=False): ## by default finds 10k, otherwise choose from quarter=1-4\n",
    "        new_filename = f\"master_{year}_{quarter}.txt\"\n",
    "        if self.find_file(new_filename, self.download_path): ## To prevent re downloading files, and make the program more efficient\n",
    "            pass\n",
    "            #print(\"Found File\")\n",
    "        else:\n",
    "        \n",
    "            master_idx_url = f\"https://www.sec.gov/Archives/edgar/full-index/{str(year)}/QTR{str(quarter)}/\"\n",
    "            self.get(master_idx_url)\n",
    "            table_element = self.find_element_by_css_selector(\"table[summary='heding']\")\n",
    "            rows_element = table_element.find_elements(By.TAG_NAME, \"tr\")\n",
    "            master_idx_link = rows_element[11].find_element(By.TAG_NAME, \"a\")\n",
    "            master_idx_link.click()\n",
    "            ## sleep to let file download\n",
    "            time.sleep(2)\n",
    "\n",
    "\n",
    "            filename = max([self.download_path + \"\\\\\" + f for f in os.listdir(self.download_path)],key=os.path.getctime)\n",
    "            print(filename)\n",
    "\n",
    "            shutil.move(filename,os.path.join(self.download_path,new_filename))\n",
    "            \n",
    "        ## strip the preceding zeroes in the string cik\n",
    "        stripped_cik = cik.lstrip(\"0\") \n",
    "        add_10q = []\n",
    "        add_10k = []\n",
    "        with open(self.download_path + \"\\\\\" + new_filename) as fp:\n",
    "            for line in fp:\n",
    "                if stripped_cik in line:\n",
    "                    if \"10-Q\" in line:\n",
    "                        add_10q.append(line) \n",
    "                    if \"10-K\" in line:\n",
    "                        add_10k.append(line)\n",
    "                        \n",
    "        if add_10k == [] and add_10q == []:\n",
    "            print(\"No filings found\")\n",
    "            return None\n",
    "                        \n",
    "        return_address = []\n",
    "        \n",
    "        if only_10k == True:\n",
    "            if add_10k == []:\n",
    "                return None\n",
    "            else:\n",
    "                for line in add_10k:\n",
    "                    return_address.append(line.split('|')[4])\n",
    "                complete_address = \"https://www.sec.gov/Archives/\" + return_address[0].strip(\"\\n\")\n",
    "                ## for now only returns complete address of first filing we find that is either a 10-K or 10-Q\n",
    "                return complete_address \n",
    "            \n",
    "                        \n",
    "        ## Single quarter archive has multiple 10-Q, or both 10-K and 10-Q reports for some company(ie. some international companies)\n",
    "        for line in add_10q:\n",
    "            return_address.append(line.split('|')[4])\n",
    "            \n",
    "        complete_address = \"https://www.sec.gov/Archives/\" + return_address[0].strip(\"\\n\")\n",
    "        return complete_address ## for now only returns complete address of first filing we find that is either a 10-K or 10-Q\n",
    "            \n",
    "        \n",
    "      \n",
    "    def find_10k_address(self, cik, year): ## Since companies may release their 10k's in different quarters\n",
    "        for i in range(1, 5):\n",
    "            address_10k = self.find_filing_address(cik=cik, year=year, quarter=i, only_10k=True)\n",
    "            if address_10k != None:\n",
    "                return address_10k\n",
    "        return None\n",
    "    \n",
    "    def filing_xml_form(self, file_add):\n",
    "        # \n",
    "        json_url = file_add.replace('-','').replace('.txt','/index.json')\n",
    "        print(json_url)\n",
    "        #son_decoded = requests.get(json_url).json()\n",
    "        response = requests.get(json_url)\n",
    "        response.raise_for_status()  # raises exception when not a 2xx response\n",
    "        if response.status_code != 204:\n",
    "            json_decoded = response.json() # We get a 403 error with a potential fix above\n",
    "               \n",
    "        for file in json_decoded['directory']['item']:\n",
    "             if file['name'] == 'FilingSummary.xml':\n",
    "                xml_ad = base_url + json_decoded['directory']['name'] + \"/\" + file['name']\n",
    "                return xml_ad  \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000789019\n"
     ]
    }
   ],
   "source": [
    "MicrosoftCorp10k = Scrape_Filing()\n",
    "ms_cik = MicrosoftCorp10k.find_cik(\"Microsoft Corp\") # use abbreviations such as Corp, Ltd, Inc, etc.\n",
    "print(ms_cik)\n",
    "file_ad = MicrosoftCorp10k.find_10k_address(ms_cik, 2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/789019/0001564590-19-027952.txt\n",
      "https://www.sec.gov/Archives/edgar/data/789019/000156459019027952/index.json\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://www.sec.gov/Archives/edgar/data/789019/000156459019027952/index.json",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ffaeca76ee6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_ad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mms_xml_ad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMicrosoftCorp10k\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiling_xml_form\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_ad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mms_xml_ad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-f5dde2ed0a7f>\u001b[0m in \u001b[0;36mfiling_xml_form\u001b[1;34m(self, file_add)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m#son_decoded = requests.get(json_url).json()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# raises exception when not a 2xx response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m204\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mjson_decoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 953\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    954\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://www.sec.gov/Archives/edgar/data/789019/000156459019027952/index.json"
     ]
    }
   ],
   "source": [
    "print(file_ad)\n",
    "ms_xml_ad = MicrosoftCorp10k.filing_xml_form(file_ad)\n",
    "print(ms_xml_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseFiling():\n",
    "    def __init__(self):\n",
    "        self.filing = dict()\n",
    "        self.filing['sec_header_content'] = {}\n",
    "        self.filing['filing_documents'] = None\n",
    "        self.test = 0\n",
    "        \n",
    "    def retrieve_filing(self, file_address):\n",
    "        response = requests.get(file_address)\n",
    "        filing = BeautifulSoup(response.content, 'lxml')\n",
    "        sec_header_tag = filing.find('sec-header')\n",
    "        \n",
    "\n",
    "        \n",
    "        display(sec_header_tag)\n",
    "        \n",
    "        # find condendsed consolidated statements of financial condition\n",
    "        #<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\" style=\"margin:auto;border-collapse:collapse; width:100%;\">\n",
    "        print(\"???\")\n",
    "        i = 0\n",
    "#         for filing_document in filing.find('document'):\n",
    "#             document_filename = filing_document.filename.find(text=True, recursive=False).strip()\n",
    "#             #print(i, \": \", document_filename)\n",
    "#             display(document_filename) \n",
    "#             i+=1\n",
    "#             #master_document_dict[document_id]['document_filename'] = document_filename\n",
    "            \n",
    "    def retrieve_xml(self, file_address):\n",
    "        base_url = xml_summary.replace('FilingSummary.xml', '')\n",
    "        content = requests.get(xml_summary).content\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        # find the 'myreports' tag because this contains all the individual reports submitted.\n",
    "        reports = soup.find('myreports')\n",
    "        master_reports = []\n",
    "        # loop through each report in the 'myreports' tag but avoid the last one as this will cause an error.\n",
    "        for report in reports.find_all('report')[:-1]:\n",
    "            #dictionary to store all the different parts we need.\n",
    "            report_dict = {}\n",
    "            report_dict['name_short'] = report.shortname.text\n",
    "            report_dict['name_long'] = report.longname.text\n",
    "            report_dict['position'] = report.position.text\n",
    "            report_dict['category'] = report.menucategory.text\n",
    "            report_dict['url'] = base_url + report.htmlfilename.text\n",
    "           \n",
    "def xml_table(self, )\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BlackRock10k = Scrape_Filing()\n",
    "#cik = BlackRock10k.find_cik(\"BlackRock Inc.\")\n",
    "cik = '0001364742'\n",
    "file_address = BlackRock10k.find_filing_address(cik, year=2019, quarter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
